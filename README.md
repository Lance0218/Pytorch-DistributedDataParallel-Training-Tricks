# Pytorch-DistributedDataParallel-Training-Tricks

This is an example that integrates **Pytorch DistributedDataParallel, Apex, warm-up, learning rate scheduler**, if you need to read this article in Chinese, please check my [Medium](https://medium.com/@fig498etyu6600/training-tricks-for-pytorch-distributed-data-parallel-1cd48cc7d97a) link.
In the following chapters, I will introduce how to use DistributedDataParallel (DDP). And how to assimilate three training techniques of Apex, warm-up, and learning rate scheduler into DDP training in order. I will also mention the set-up of early-stop and Random seed.

## DistributedDataParallel (DDP)

1. Pytorch official website also recommends using DistributedDataParallel (multi-process control multi-GPU) instead of DataParallel (single-process control multi-GPU) when using    multi-GPU training, which improves the speed and solves the problem of uneven GPU loading.
2. The basic usage is to load the model to be used and wrap it with DDP, local_rank is the rank of the current GPU generated by calling torch.distributed.launch.
3. Data transmission under multiple nodes will seriously affect efficiency, so DistributedSampler is used to ensure that DataLoader will only load a specific subset of the data    set. The batch_size under DistributedSampler is the actual batch size used by a single GPU.
4. Call set_epoch(epoch) at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs.
```
from torch.nn.parallel import DistributedDataParallel as DDP

# Load MNIST
data_root = './'
train_set = MNIST(root=data_root, download=True, train=True, transform=ToTensor())
train_sampler = DistributedSampler(train_set)
same_seeds(args.seed_num)
train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=(train_sampler is None), pin_memory=True, sampler=train_sampler)
valid_set = MNIST(root=data_root, download=True, train=False, transform=ToTensor())
valid_loader = DataLoader(valid_set, batch_size=args.batch_size, shuffle=False, pin_memory=True)

# Load model
same_seeds(args.seed_num)
model = Toy_Net()
model = model.to(args.local_rank)

# Build training model
parallel_model = DDP(model, device_ids=[args.local_rank], output_device=args.local_rank)

# Train model
for epoch in range(args.epochs):
    #...
    train_sampler.set_epoch(epoch)
    for image, target in tqdm(train_loader, total=len(train_loader)):
    #...
```
